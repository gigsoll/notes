# 1. Що таке EDA і які його основні цілі?
EDA (Explorary data analysis) – це процес розвідувального аналізу даних, основними цілями якого є ознайомлення з даними, знаходження основних закономірностей у їх розподілі, виявлення аномалій та викидів, та створення первинних гіпотез
# 2. Які основні кроки включає в себе процес EDA?
Основними кроками розвідувального аналізу даних є:
1. підрахунок кількісних показників у датасеті
2. Побудова графіків для аналізу даних
3. Побудова описової статистики (min, max, avg, med)
4. Окремий аналіз кожної ознаки, в основному який в неї закон розподілу
5. Огляд та порівняння ознак у тестовому тренувальному та валідаційному датасетах
6. Кореляційний аналіз
7. Регресійний аналіз
8. Пошук викидів та аномалій
9. Аналіз закономірностей методом кластеризації, факторного аналізу та зменшення розмірності
10. Виявлення періодичності у часових рядах
# 3. Які інструменти і техніки використовуються для проведення EDA?
Основними інструментами є pandas, matplotlib, seaborn, саме з їх допомогою проводиться найпервинніший аналіз даних, надалі можна використовувати інші бібліотеки, такі як `PandasProfiling` (виявлення впливу одних ознак на інші), `AutoViz` (автоматична побудова графіків), `SweetViz` (побудова гістограм, та зв'язки між часовими рядами). 
# 4. Які види даних можна використовувати для проведення EDA?
Для розвідувального аналізу даних найкраще використовувати числові данні, адже з їх допомогою можна проводити розрахунки того як данні розподілені, визначати кореляцію між ними, будувати графіки і так далі. Також можуть бути числові дані, але їх зазвичай додатково обробляють та дані по часу, але вони часто використовуються надалі у інженерії ознак
# 5. Які основні методи візуалізації даних використовуються в EDA?
Основні методи візуалізації даних:
- Побудова діаграм
- Побудова гістограм
- Побудова теплових карт (для розподілу)
- Побудова графіків розсіювання
- Box-plot (під питанням)
# 6. Що таке кореляційний аналіз і як він використовується в EDA?
Кореляційний аналіз це аналіз основною ціллю якого є виявлення залежностей між даними, та визначення сили цієї залежності. В EDA він використовується в основному для того щоб виявити сильно залежні одна від одної ознаки та відкинути їх, особливо коли сила залежності в них наближається до прямої
# 7. Як проводиться аналіз розподілу даних під час EDA?
Під час EDA в першу чергу відбувається спостереження за тим чи дані підлягають нормальному закону розподілу, чи ні, також розглядається те, наскільки сильно дані розкидані по діапазону, відбувається перевірка кількості пропущених ознак, розгляд того чи є аномалії чи викиди у даних.
# 8. Які методи використовуються для виявлення та обробки пропущених значень в даних під час EDA?
З пропущеними даними можна впоратися двома шляхами — відкинути їх або замінити на дані за припущенням (можна взяти середнє значення величини, середнє значення між сусідами) виявити їх можна з допомогою вбудованих бібліотек pandas, або з допомогою графіків
# 9. Як використовуються статистичні показники, такі як середнє, медіана, мода, дисперсія, у процесі EDA?
Всі ці показники використовуються на початковому етапі розвідувального аналізу даних для того щоб отримати загальну інформацію про дані.
# 10. Як проводиться аналіз викидів (outliers) в даних під час EDA?
Для виявлення викидів даних використовують 3 основних методи — аналіз по квартилях, огляд графіків та з використанням спеціальних бібліотек для часових рядів. 
# 11. Як проводиться порівняльний аналіз між категоріальними змінними в процесі EDA?
Категоріальні дані зазвичай порівнюють між собою з допомогою різних діаграм а також використовують для того щоб побачити як сильно вони корелюють з іншими величинами. На початку їх перетворюють на числові дані з допомогою словника чи спеціалізованих бібліотек, а їх вже можна використовувати з іншими числовими чи категоріальними даними для виявлення кореляцій
# 12. Які методи використовуються для виявлення та врахування взаємозв'язку між змінними під час EDA?
# 13. Як використовується графічний аналіз (plotting) для дослідження даних в процесі EDA?
Під час EDA графовий аналіз використовується для того щоб знайти закономірності в даних, залежність між даними, виявити пропущені значення та викиди
# ==14. Як проводиться сегментація даних в процесі EDA і для чого вона використовується?==
Сегментації даних у EDA немає, є кластеризація даних. Кластеризація даних — це групування схожих об'єктів, або кластеризація на основі певної властивості. Вона використовується для того щоб знайти приховані структури даних. Також кластеризовані дані використовують для пошуку асоціацій між ними
# 15. Як проводиться аналіз часових рядів (time series analysis) в рамках EDA?
Під час EDA основне що відбувається з часовими рядами — це пошук пропущених значень та їх усунення адже для моделей машиного навчання важливо щоб в часових рядах не було прошусків, виявлення сезонності та іншої залежності даних від часу 
# 16. Що таке Feature Engineering і чому він важливий у процесі машинного навчання?
Feature Engeniring –  це процес визначення важливості ознак створення нових ознак та видалення зайвих. Він важливий у процесі машинного навчання тому що від правильно підібраних ознак залежить результат машинного навчання. Для машиного навчання важливо щоб ознак було достатньо та щоб не було пропусків даних, також важливо щоб ознаки мали в собі важливу інформацію та не були детерміновані одна від одної
# 17. Які є основні завдання Feature Engineering?
Основними завданнями Feature Engeneering є — виявлення та видалення малоінформативних ознак, виявлення та видалення сильно корельованих чи детермінованих ознак (0.95 — 1), виявлення та видалення витоків, аналіз важивостей ознак, створення нових ознак, модифікація вже існуючих ознак
# 18. Які види даних можуть бути використані для створення нових ознак у процесі Feature Engineering?
Можуть використовуватися майже будь-які дані. Числові, категоріальні, текстові, часові дані
# 19. Які основні методи перетворення категоріальних ознак у числові у процесі Feature Engineering?
Основним методом перетворення категоріальних даних в числові є використання словників та автоматичного розмітка категорій автоматично з допомогою автоматичних заголовків.
# 20. Що таке уявні (dummy) ознаки і як вони використовуються в Feature Engineering?
Уявні ознаки — числові значення які утворені з категоріальних ознак. Часто використовуються у процесі машинного навчання коли модель не може ефективно обробити категоріальні дані. Тоді дані категорії приєднуються до певного числа і модель працює не з категорією, а лише зі числовим значенням яке їй присвоєно
# 21. Які основні методи кодування категоріальних ознак в числові у процесі Feature Engineering?
- One-Hot кодування — кожна змінна перетворюється в бінарне значення яке вказує чи належить це значення до певної категорії чи ні з допомогою 0 та 1
- Кодування за заголовками — застосовується переведення текстових даних у текст за словником  
- Порядкове кодування — кодування категорій в цифри відбувається в за певним правилом
- Кодування за частотою — Частота появи в датасеті визначає номер категорії
- Кодування за цільовим значенням – Для кодування категорії використовується значення цільової ознаки що відповідає яким ця категорія відповідає. Наприклад для категорії може призначатися середнє значення цільової ознаки як кодова цифра
# 22. Які методи можна використовувати для обробки пропущених значень в ознаках під час Feature Engineering?
Найпоширенішим методом боротьби з пропущеними значеннями є приписування пропущеним значенням значення які мають сенс виходячи з уже існуючих даних. Для цього може використовуватися середнє арифметичне, медіана, дані з попередніх спостережень, константне стандартне значення, використання регресорів чи класифікаторів. Також можливо видаляти ознаки, якщо кількість пропусків дуже велика. Ще одним способом є використання індикаторів пропущених значень — створюється нова ознака яка має запис про те чи є пропущене значення чи ні у іншій ознаці.
# 23. Які техніки можна використовувати для вилучення важливих ознак з великих наборів даних у процесі Feature Engineering?
Для того щоб вилучити не важливі ознаки потрібно визначити важливість ознак для моделі машинного навчання, для цього слід використовувати графік важливості ознак. Якщо ознака має низьку важливість і часто зузстрічається, її потрібно вилучити як шум. Також ознаки які сильно корелюють одна з одною чи з таргетом слід вилучати. Ознаки які не несуть важливої інформації підлягають видаленню. 
# 24. Як відбувається створення нових ознак на основі існуючих у процесі Feature Engineering?
Для створення нових ознак можна використовувати наступні техніки
- Створення нових ознак на основі знань з галузі
- Виконання математичних операцій над функціями (додавання, множення, ділення, отримання коренів)
- Використання статистичних методів
- Кодування категоріальних даних
- Розкладання ознаки
# 25. Які методи можна використовувати для масштабування ознак у процесі Feature Engineering?
Нормалізація та стандартизація.
# ==26. Що таке інженерія взаємодії ознак (feature interaction engineering) і як вона використовується в Feature Engineering?==
Як слідує з назви — це процес створення нових ознак на основі уже існуючих з допомогою математичних перетворень. Зазвичай цей метод використовується для створення нової ознаки яка відображає те як дві чи більше ознак взаємодіють одна з одною. Він може використовуватися як для зменшення розмірності датасету за рахунок створення нової ознаки яка буде одночасно відображати декілька ознак і при цьому буде більш інформативною так і для видалення ознак які мають залежність по формулі до іншої.
# ==27. Як використовуються додаткові джерела інформації для створення нових ознак у процесі Feature Engineering?==
Додаткові джерела даних можуть бути використані для того щоб розширити кількість інформації та створити нові ознаки що можуть бути корисними для моделі машиного навчання надалі. Це можуть бути дані із зовнішніх джерел, які не пов'язані на пряму з ключовою ознакою, так і додаткові данні що базуються на предметній області. Також можуть створюватися перехресні ознаки які є комбінацією рівнів двох ознак.
# 28. Як використовується категоріальна взаємодія (categorical interaction) для створення нових ознак у Feature Engineering?
Категоріальні дані так само як і числові можуть взаємодіяти між собою, проте вони впливають один на одного не по формулі (множення додавання і так далі) а зазвичай для визначення взаємодії категорій між собою та з таргетом. Можуть використовуватися також частота появи категорій перемножена одна на одну.
# 29. Які інструменти та бібліотеки використовуються для проведення Feature Engineering в Python?
- Sklearn
- SHAP
- LIME
# 30. Як провести валідацію нових ознак у процесі Feature Engineering?
Валідація може бути проведена за допомогою побудови графіків важливості ознак та з його допомогою перегляду того чи є новостворені ознаки важливими під час навчання моделі. Також можна провести тренування моделі на дататсеті з відсутніми новими ознаками та з ними і побачити яким чином змінилася точність моделі. Якщо точність моделі зростає то нові ознаки є корисними
# 31. Що таке ансамблеві методи машинного навчання, і яку роль вони відіграють у підвищенні ефективності моделей?
Ансамблеві методи - це парадигма машинного навчання, де кілька моделей (часто званих «слабкими учнями») навчаються для вирішення однієї і тієї ж проблеми і об'єднуються для отримання кращих результатів. Основна ідея полягає в тому що можна використовувати прості моделі як будівельні блоки для створення більш складних
# 32. Як ви розумієте поняття "boosting" у контексті машинного навчання?
Бустінг — це стратегія машиного навчання з учителем, яка як основний підхід має поєднання декількох слабких моделей у одну більш складну, та прогремивну що здатна робити кращі передбачення. Основною перевагою над іншими підходами є те що робиться акцент на не правильно класифікованих зразках. Неправильно класифіковані зразки передаються наступним моделям для того щоб вони виправили ці помилки і таким чином створили більш прогресивну модель
# 33. Які основні ідеї стоять за алгоритмом Gradient Boosting?
Основними ідеями градієнтного бустингу є адаптація моделей до залишкових помилок попередніх моделей. Він мінімізує функцію втрат за допомогою градієнтного спуску і може бути застосований як до задач регресії, так і до задач класифікації.
# 34. Як працює алгоритм Gradient Boosting Machine (GBM)?
Алгоритм GBM навчає послідовно композитну модель, яка складається з набору дерев рішень (або регресійних дерев). При цьому всі дерева навчаються на одному і тому ж наборі даних. Процес навчання відбувається ітеративно: На початку всім даним призначається однакова вага, навчається перше рішення і утворюється градієнт помилок, і в даних на яких є найбільша помилка збільшується вага для наступних дерев, таким чином виконується і надалі поки не буде досягнуто максимальної точності
# 35. Які є основні переваги та недоліки використання Gradient Boosting у порівнянні з іншими методами?
**Переваги**
- Висока точність
- Добре справляється з задачами класифікації та регресії
- Здатен працювати з пропущеними значеннями
**Недоліки**
- Вимагає багато комп'ютерної потужності
- Якщо є дуже багато даних його можна легко перетренувати
- Є гірше інтерпретованою в порівнянні з іншими
# 36. Що таке алгоритм XGBoost, і які його особливості порівняно з іншими методами boosting?
Алгоритм XGBoost є покращеною версією GBM  і працює за подібним принципом, але у порівнянні з ним має декілька значних доповнень які пришвидшують його роботу та точність. Від так він має паралельне передоброблення даних що пришвидшує його роботу, та має багато регулюризації для даних, щоб запобігти перенавчанню також там присутнє кешування даних. В загальному це пришвидшена та покращена версія GBM
# 37. Які є ключові гіперпараметри, які можна налаштовувати у XGBoost, і як вони впливають на якість моделі?
- booster – вказує модель яка буде використовуватися для кожного проходу (bgtree, dart – дерева, gblinear – лінійна), в більшості випадків бустер на деревах працює краще за лінійний
- eta – розмір зменшення кроку який робиться по градієнту
- gamma – мінімальне зменшення втрат, необхідне для здійснення поділу
- max_depth – задає максимальну глибину дерева рішень, дозволяє навчити модель вивчати відношення між специфічними параметрами, але занадто високе може призводити до перетренування
- subsample – задає яка частина датасету буде віддаватися кожному дереву для навчання
- alpha, lambda – задає методи регуляризації даних
# 38. Як працює алгоритм LightGBM, і що робить його особливим у порівнянні з іншими бібліотеками boosting?
LightGBM є більш оптимізованим та простим алгоритмом і будує не повне дерево, а лише частину яка за припущенням алгоритму матиме найменшу кількість втрат даних та помилкових прогнозів.
# 39. Які є основні відмінності між XGBoost та LightGBM?
LightGBM у порівнянні з XGBoost має багато спільних рис таких як регуляризація даних, більша ефективність, проте основна перевага в тому що LightGBM будує не повне дерево, ряд за рядом, а лише дерево тих листків які за припущенням алгоритму будуть надавати найменшу похибку.
# 40. Які основні переваги та недоліки використання LightGBM у порівнянні з іншими методами boosting?
**Переваги**
- Швидше тренується на великих наборах даних
- Менше споживання пам'яті
- Доволі висока якість передбачень
**Недоліки**
- Передбачення здійснюються повільно, бо дерева не широкі, а вузлькі
- Високий ризик оверфітінга, особливо з малими датасетами
- Менша гнучкість
# 41. Як вибрати найкращий алгоритм boosting для вашої задачі машинного навчання?
- Якщо набір даних не великий — AdaBoost
- Якщо Потрібна інтерпретована модель — GBM
- Для великого набору даних і потужного комп'ютера — XGBoost
- Для великого набору даних і слабкого комп'ютера — LightGBM
# 42. Що таке регуляризація у контексті алгоритмів boosting, і як вона може бути застосована для уникнення перенавчання?
регуляризація – прийом в машинному навчанні для мінімізації значень параметрів моделі з метою покращення її узагальнювальної здатності та уникнення перенавчання. В основному він вводиться таким чином що моделі призначається штраф за збільшення складності на певному етапі навчання, щоб вона не вивчала шум
# 43. Як проводиться налаштування гіперпараметрів у моделях boosting, і які стратегії можуть бути використані для цього?
Кислий борщ мені в стаканчик, манав я це відповідати
# 44. Як використовуються ансамблеві моделі типу boosting для задач регресії та класифікації?
використання ансамблевих моделей на основі бустингу для задач регресії та класифікації:
Регресія:
- Будується ансамбль дерев регресії
- Кожне наступне дерево намагається зменшити помилку попередніх на залишках (residuals)
- Остаточний прогноз - сума прогнозів всіх дерев
Класифікація:
- Будується ансамбль дерев рішень
- Кожне наступне дерево фокусується на важких прикладах, збільшуючи їх ваги
- Остаточна класифікація базується на голосуванні/усередненні прогнозів дерев
# 45. Як використовувати результати аналізу важливості ознак (feature importance) для покращення роботи моделей boosting?
Результати дослідження важливості ознак використовуються в boosting для того щоб визначити які критерії є більш чи менш важливими при прийнятті рішень моделлю, і якщо потрібно для зміни вагів у моделі для зміни результатів. Також це може використовуватися щоб регулювати мінімальну похибку необхідну для дерева, налаштування глибини дерева рішень, та регулювання кроку по градієнту спуску. Також аналіз важливості ознак може підвищити інтерпретованість моделі
# 46. Які кроки потрібно виконати для завантаження набору даних перед початком EDA?
1. Обрати датасет
2. Імпортувати бібліотеки numpy та pandas
3. Завантажити датасет з допмогою pandas
# 47. Як перевірити загальну структуру даних (рядки, стовпці) за допомогою Python?
`df.dtypes`
# 48. Які методи можна використовувати для першого огляду даних?
Можна перевірити розміри датасету та з допомогою `df.shape()`, перевірити чи є порожні значення у рядках `df.isna().sum()`, отримати типи даних з `df.dtypes`, отримати загальну описову статистику про кожну властивість з допомогою `df.describe(`
# 49. Як оцінити розмір та обсяг набору даних (кількість рядків та стовпців)?
`df.shape()`
# 50. Як перевірити наявність пропущених значень у наборі даних?
`df.isna().sum()`
# 51. Які методи можна використовувати для обробки пропущених значень?
- видалити пропущенні рядки `df.dropna(inplace=True)`
- заповнити пропуски константою `df.fillna(0, inplace=True)`
- заповнити пропуски середнім або медіанним значенням `df.fillna(mean, inplace=True)`
- додати колонку індикатор пропущених значень `df['col1_missing'] = df['col1'].isna()`
# 52. Як оцінити розподіл кількісних змінних у наборі даних?
Це можна зробити двома шляхами — побудувати графік розподілу змінної, або з допомогою спостереження за описовою статистикою
# 53. Як визначити розподіл категоріальних змінних та їх унікальні значення?
Для категоріальних змінних можна отримати частоту зустрічі та побудувати графік розподілу цих категорій з допомогою `df[column].value_counts()` а унікальні значення окремо можна отримати з допомогою `df.unique()`
# 54. Як оцінити взаємозв'язок між кількісними змінними у наборі даних?
оцінити його можна з допомогою `df.corr()` для побудови кореляційної матриці. також її можна перетворити в плот типу теплова карта для того щоб мати кращу наочність.
# 55. Як виявити аномальні значення або викиди в наборі даних?
Виявити аномалії чи викиди можна трьома основними шляхами — спостереження за графіком величин, аналіз за квартилями (все що виходить за 95% або 5% скоріш за все є викидом чи аномалією) та використання специфічних бібліотек для часових рядів
# 56. Які методи можна використовувати для виявлення взаємозв'язку між кількісними та категоріальними змінними?
можна використати кореляційну матрицю, або групування за числовою та категоріальною змінно. `df.corr()` та `df.groupby()`
# 57. Як відобразити залежність між двома кількісними змінними за допомогою діаграми розсіювання (scatter plot)?
Найкраще це зробити з допомогою `plt.scater()` з її допомогою можна дуже просто задати величини для яких будувати scatter plot та налаштувати зовнішній вигляд графіка
# ==58. Як оцінити розподіл змінних за категоріями?==
# 59. Як перевірити наявність дублікатів у наборі даних та їх обробити?
Для того щоб перевірити наявність дублікатів у наборі даних слід використати  `df.duplicated().sum()`. Якщо потрібно більш детально ознайомитися — `df.duplicated()`
# 60. Як підготувати звіт з результатами EDA та які основні висновки можна зробити після аналізу набору даних?
Звіт по результах EDA можна розробити використовуючи matplotlib pandas та numpy, autoviz, swetpviz та pandas profiling
# 61. Які основні завдання включає в себе feature engineering?
- Оцінка та огляд вже існуючих ознак
- Синтез нових ознак
- Видалення зайвих — малоінформативних ознак
# 62. Як перевірити наявність пропущених значень у наборі даних та їх обробити перед виконанням feature engineering?
`df.isna().sum()` — виявлення пропущених даних
- видалити пропущенні рядки `df.dropna(inplace=True)`
- заповнити пропуски константою `df.fillna(0, inplace=True)`
- заповнити пропуски середнім або медіанним значенням `df.fillna(mean, inplace=True)`
- додати колонку індикатор пропущених значень `df['col1_missing'] = df['col1'].isna()`
# 63. Як перевірити типи даних кожної змінної та чи вони відповідають їхньому семантичному значенню?
`df.dtypes`
# 64. Як перетворити категоріальні змінні у числові для подальшого використання в моделях машинного навчання?
їх можна перетворити різними методами:
- Закодувати з допомогою словника
- Згенерувати заголовки автоматично але без словника
- Закодувати Впорядкувати за частотою
- Закодувати за відношенням до таргету
- Використати One-Hot кодування
# 65. Як виконати кодування змінних методом One-Hot Encoding та чому це корисно для моделювання?
One-Hot кодування — це кодування категоріальних змінних, яке в основі має створення декількох додаткових ознак які вказують чи належить ця категорія до певної іншої категорії. Це може бути корисним для того щоб вказати більш складні категорії та щоб спростити обробку даних для моделі машиного навчання
# 66. Як обробити змінні з високою кількістю унікальних категорій?
Їх потрібно закодувати з допомогою One-Hot кодування або кодування за допомогою кодування таргета
# 67. Як створити нові ознаки (фічі) на основі існуючих змінних?
Найпростішим методом є виконання певних математичних операцій з уже існуючими ознаками. Від так довжину та ширину можна перемножити та отримати площу, або з маси тіла та ваги — отримати індекс маси тіла. Також у часових рядах можна зробити розбивку даних по сезонах (певним часовим проміжках). Також можливе використання статистичних методів
# 68. Як використовувати методи розмірності зменшення для створення нових ознак?
Зміна розмірності даних необхідна для полегшення роботи моделі машинного навчання. Моделі простіше обробляти данні які мають менший розмір, тому дуже часто зменшення розміру даних використовують для створення нової ознаки
# 69. Як обробити числові змінні, щоб покращити їхню інтерпретованість та корисність для моделі?
- Зменшити їх розмірність щоб вони поміщалися в діапазоні від -1 до 1
- Якщо розкид даних дуже широкий, а розмірність зменшувати не хочеться можна згрупувати дані по певних діапазонах
- Можна створити нові ознаки які відображатимуть статистичні особливості даних
- Данні слід нормалізувати
- Прибрати викиди та аномалії
- Прибрати порожні значення, або якось їх замінити
# 70. Як вирішити проблему викидів (outliers) у числових змінних перед виконанням feature engineering?
Існує два способи бородьби з викидами 
1. Видалення викидів
2. Заміна викидів на + або - нескінченність
3. Логарифмічне перетворення даних що може прибрати викиди
# 71. Як використовувати методи нормалізації та стандартизації для числових змінних?
Нормалізація — це процес роботи з даними який зменшує їх відхилення та приводить до спільного координатного простору, за формулою (x - xmin) / (xmax - xmin)
Стандартизація — це операція подібна до нормалізації, проте вона працює за іншою формулою. Вона використовує формулу (x - xсереднє) / середнє квадратичне відхилення
Такі операції застосовуються до властивостей на процесі передоброблення даних для зменшення розмірності даних та для того щоб забезпечити більш стандартизовану роботу для моделі машинного навчання
# 72. Як створити функціональні ознаки на основі даних про час або дату?
Самі ознаки про дату або час не варто віддавати для моделі машинного навчання, адже вони можуть бути шкідливими для передбачень. Найкращим варіантом це створити нові ознаки. З набору даних, де присутні часові дані, можна виділити певні часові періоди, такі як тижні місяці, можна також створити властивості які будуть вказувати на пікові періоди 
# 73. Як видалити незначущі або дубльовані ознаки під час процесу feature engineering?
Спочатку потрібно визначити ознаки які потрібно видалити, це можна зробити з допомогою графіка важливості ознак. Після цього можна використати два методи 
- `df.drop()`
- `df[["перший", "набір", "властивостей"]] = df[["набір"]]` 
# 74. Як створити взаємодіючі або поліноміальні ознаки для змінних, щоб врахувати неявні взаємозв'язки?

# 75. Як оцінити ефективність застосованих методів feature engineering та їх вплив на модель машинного навчання?
Вплив нових ознак на модель машинного навчання можна дослідити з допомогою використання графіка важливості ознак та оцінки моделі. Якщо нові ознаки є важливими на графіку важливостей ознак, значить вони сильно впливають на модель, а якщо ще й результати передбачень покращуються — вони є ефективними
# 76. Що таке ансамбль моделей, і які переваги надає використання ансамблю алгоритмів в порівнянні з окремими моделями?
Ансамблеві методи - це парадигма машинного навчання, де кілька моделей (часто званих «слабкими учнями») навчаються для вирішення однієї і тієї ж проблеми і об'єднуються для отримання кращих результатів. Основна ідея полягає в тому що можна використовувати прості моделі як будівельні блоки для створення більш складних
# 77. Які є основні алгоритми boosting, і як вони відрізняються один від одного?
- AdaBoost – працює подібно до градієнтного але не використовує градієнтний спуск, через що допускає більше помилок
- Gradient Boosting (GBM) — Навчає дерева рішень з використанням помилок попередніх моделей, та градієнтний спуск для того щоб моделі допускали менше помилок надалі
- LightGBM — в основному працює подібно до XGBoost та GBM але заміть того щоб будувати дерево рішень в ширину, будує дерево в глибину з листями які за версією моделі повинні призвести до найменшої кількості помилок
- XGBoost – покращена версія GBM, має стандартизацію даних, кешування та паралельну обробку даних
# 78. Які гіперпараметри впливають на ефективність алгоритмів boosting, і як їх налаштувати?
- Кількість дерев (**n_estimators**)
- Розмір кроку (**eta**)
- Глибина дерева (**max_depth**)
- loose function
# 79. Як використовувати алгоритми boosting для задач класифікації?
Класифікація:
- Будується ансамбль дерев рішень
- Кожне наступне дерево фокусується на важких прикладах, збільшуючи їх ваги
- Остаточна класифікація базується на голосуванні/усередненні прогнозів дерев
# 80. Як використовувати алгоритми boosting для задач регресії?
Регресія:
- Будується ансамбль дерев регресії
- Кожне наступне дерево намагається зменшити помилку попередніх на залишках (residuals)
- Остаточний прогноз - сума прогнозів всіх дерев
# 81. Як вирішується проблема перенавчання при використанні алгоритмів boosting?
Проблему перенавчання можна виправити трьома основними способами — Зменшити кількість кроків, налаштувати стандартизацію першого та другого рівня та відредагувати параметри для того щоб прибрати проблеми перенавчання (наприклад прибрати дані про час з навчального датасету). Також, якщо цього було не зроблено — розділити датасет на навчальний, тестовий та валідаційний
# 82. Які є переваги та недоліки алгоритмів boosting порівняно з іншими методами машинного навчання?
**Переваги**
- Висока якість прогнозів
- Дуже добре працюють з великою кількістю даних
- Підходять як для регресії так і для класифікації
- Мають якісну документацію
**Недоліки**
- Вимагають багато пам'яті
- Вимагають багато обрахункових потужностей
- Прогнози можуть бути повільними
- Висока чутливість до перенавчання
# 83. Як використовувати перехресну перевірку (cross-validation) для налаштування параметрів алгоритмів boosting?
Перехресна перевірка — це стратегія перевірки моделі машинного навчання яка працює наступним чином — датасет розбивається на тренувальний та тестовий за певною пропрцією (80 до 20) потім відбувається навчання та валідація, наступним кроком — відбувається зсув даних і тепер інша частина загального датасету є тренувальною, а інша тестовою, і так поки весь датасет не побуває тестовим. Це довзоляє підвищити оцінку точності прогнозів, прибрати похибки, та пересвідчитися що модель працює правильно. Така оцінка дозволяє покращити налаштування гіперпараметрів шляхом кращої перевірки

![](https://i.imgur.com/VkQHeRG.png)

# 84. Як розпізнати ознаки перенавчання при роботі з алгоритмами boosting?
В основному перенавчання виражається занадто якісними прогнозами (90 — 100%). Опимумом є 70 —85 відсотків точності
# 85. Як здійснюється вагове оновлення при побудові ансамблю за допомогою алгоритмів boosting?
Спочатку всі моделі навчання ініціалізуються з однаковою вагою, після кожної ітерації навчання відбувається аналіз помилок та на її основі — змінюються ваги. Таким чином відбувається певну кількість разів (задається в гіперпараметрах) і в кінці отримується бустінг модель яка є найкраще навчаною з парвильно розставленими вагами. В різних алгоритмах зміна ваги відбувається по різному. Наприклад XGBoost змінює ваги пропорційно до помилки, а AdaBoost – обернено пропорційно
# 86. Як використовувати алгоритми boosting для вирішення задачи ранжування?
Зазвичай задача ранджування перетворюється на задачу класифікації. Створюється кількість класів в залежності від пріоритетету, потім до таких чином оброблених даних застосовується модель бустінгу як до звичайної задачі класифікації
# 87. Як використовувати алгоритми boosting для детекції аномалій (аномалійних подій) у даних?
# 88. Як робиться вагове голосування моделей в ансамблі за допомогою алгоритмів boosting?
# 89. Як використовувати регуляризацію для уникнення перенавчання при використанні алгоритмів boosting?
регуляризація – прийом в машинному навчанні для мінімізації значень параметрів моделі з метою покращення її узагальнювальної здатності та уникнення перенавчання. В основному він вводиться таким чином що моделі призначається штраф за збільшення складності на певному етапі навчання, щоб вона не вивчала шум. Оскільки машині вводиться штраф — вона понижує ваги для моделей які беруть до уваги багато даних, які є зашумленими
# 90. Які інструменти та бібліотеки можна використовувати для реалізації алгоритмів boosting в Python?
- scikit-learn
- XGBoost
- LightGBM
- sklearn